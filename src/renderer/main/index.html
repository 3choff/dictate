<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Dictate</title>
    <link rel="stylesheet" href="../main/styles.css">
</head>
<body>
    <div class="container">
        <div class="top-bar">
            <div class="drag-indicator-container">
                <div class="drag-indicator"></div>
            </div>
            <div class="close-button" id="close-btn">
                <svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M1 1L11 11M1 11L11 1" stroke="white" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
            </div>
        </div>
        <div class="main-content">
            <div class="icon-button settings-icon">
                <svg viewBox="0 0 32 32" enable-background="new 0 0 32 32" id="Editable-line" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"><circle cx="16" cy="16" fill="none" id="XMLID_224_" r="4" stroke="#e8eaed" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2"></circle><path d=" M27.758,10.366l-1-1.732c-0.552-0.957-1.775-1.284-2.732-0.732L23.5,8.206C21.5,9.36,19,7.917,19,5.608V5c0-1.105-0.895-2-2-2h-2 c-1.105,0-2,0.895-2,2v0.608c0,2.309-2.5,3.753-4.5,2.598L7.974,7.902C7.017,7.35,5.794,7.677,5.242,8.634l-1,1.732 c-0.552,0.957-0.225,2.18,0.732,2.732L5.5,13.402c2,1.155,2,4.041,0,5.196l-0.526,0.304c-0.957,0.552-1.284,1.775-0.732,2.732 l1,1.732c0.552,0.957,1.775,1.284,2.732,0.732L8.5,23.794c2-1.155,4.5,0.289,4.5,2.598V27c0,1.105,0.895,2,2,2h2 c1.105,0,2-0.895,2-2v-0.608c0-2.309,2.5-3.753,4.5-2.598l0.526,0.304c0.957,0.552,2.18,0.225,2.732-0.732l1-1.732 c0.552-0.957,0.225-2.18-0.732-2.732L26.5,18.598c-2-1.155-2-4.041,0-5.196l0.526-0.304C27.983,12.546,28.311,11.323,27.758,10.366z " fill="none" id="XMLID_242_" stroke="#e8eaed" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="2"></path></g></svg>
            </div>
            <div class="mic-button">
                <svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path fill-rule="evenodd" clip-rule="evenodd" d="M14.5 10.5V5.5C14.5 4.11929 13.3807 3 12 3C10.6193 3 9.5 4.11929 9.5 5.5V10.5C9.5 11.8807 10.6193 13 12 13C13.3807 13 14.5 11.8807 14.5 10.5ZM12 1C9.51472 1 7.5 3.01472 7.5 5.5V10.5C7.5 12.9853 9.51472 15 12 15C14.4853 15 16.5 12.9853 16.5 10.5V5.5C16.5 3.01472 14.4853 1 12 1Z" fill="#8ab4f8"></path> <path d="M12 17C5.49999 17 5.99999 12 5.99999 12C5.99999 12 6.00001 11 5.00001 11C4.00001 11 3.99999 12 3.99999 12C3.99999 12 3.54013 18.4382 11 18.9657V22C11 22.5523 11.4477 23 12 23C12.5523 23 13 22.5523 13 22V18.9657C20.4599 18.4382 20 12 20 12C20 12 20 11 19 11C18 11 18 12 18 12C18 12 18.5 17 12 17Z" fill="#8ab4f8"></path> </g></svg>
            </div>
            <div class="icon-button sparkle-icon" title="Correct selected text">
                <svg width="20" height="20" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
                  <g>
                    <path fill="#e8eaed" d="M247.355,106.9C222.705,82.241,205.833,39.18,197.46,0c-8.386,39.188-25.24,82.258-49.899,106.917 c-24.65,24.642-67.724,41.514-106.896,49.904c39.188,8.373,82.254,25.235,106.904,49.895c24.65,24.65,41.522,67.72,49.908,106.9 c8.373-39.188,25.24-82.258,49.886-106.917c24.65-24.65,67.724-41.514,106.896-49.904 C315.08,148.422,272.014,131.551,247.355,106.9z"></path>
                    <path fill="#8ab4f8" d="M407.471,304.339c-14.714-14.721-24.81-40.46-29.812-63.864c-5.011,23.404-15.073,49.142-29.803,63.872 c-14.73,14.714-40.464,24.801-63.864,29.812c23.408,5.01,49.134,15.081,63.864,29.811c14.73,14.722,24.81,40.46,29.82,63.864 c5.001-23.413,15.081-49.142,29.802-63.872c14.722-14.722,40.46-24.802,63.856-29.82 C447.939,329.14,422.201,319.061,407.471,304.339z"></path>
                    <path fill="#8ab4f8" d="M146.352,354.702c-4.207,19.648-12.655,41.263-25.019,53.626c-12.362,12.354-33.968,20.82-53.613,25.027 c19.645,4.216,41.251,12.656,53.613,25.027c12.364,12.362,20.829,33.96,25.036,53.618c4.203-19.658,12.655-41.255,25.023-53.626 c12.354-12.362,33.964-20.82,53.605-25.035c-19.64-4.2-41.251-12.656-53.613-25.019 C159.024,395.966,150.555,374.351,146.352,354.702z"></path>
                  </g>
                </svg>
            </div>
        </div>
    </div>
    <script src="../../shared/language-map.js"></script>
    <script src="../../shared/deepgram.js"></script>
    <script>
      document.addEventListener('DOMContentLoaded', () => {
        const closeBtn = document.getElementById('close-btn');
        const settingsIcon = document.querySelector('.settings-icon');
        const helpButton = document.querySelector('.help-icon');
        const sparkleButton = document.querySelector('.sparkle-icon');
        const micButton = document.querySelector('.mic-button');
        let mediaRecorder;
        let audioChunks = [];
        let dgSocket = null;
        let dgActive = false;
        // Groq (Whisper) simple silence-based segmenter state
        let groqActive = false;
        let groqAudioCtx = null;
        let groqSource = null;
        let groqProcessor = null;
        let groqSamples16k = []; // plain Number[] of int16 samples at 16kHz
        let groqLastBoundary = 0; // index in 16k sample domain
        let groqInSilenceMs = 0;
        let groqHadSpeech = false;
        const GROQ_SILENCE_MS = 1000; // silence of 1s
        const GROQ_SILENCE_DB = -30; // classify more as silence
        const GROQ_MAX_SEGMENT_MS = 15000; // safety cut at 15s
        const GROQ_MIN_SEGMENT_MS = 200; // ignore ultra-short noise-only

        function dbfsFromRms(rms) {
            if (rms <= 1e-9) return -120;
            return 20 * Math.log10(rms);
        }

        function mixToMonoFloat32(audioBuffer) {
            const ch = audioBuffer.numberOfChannels;
            if (ch === 1) {
                return audioBuffer.getChannelData(0);
            }
            const len = audioBuffer.length;
            const out = new Float32Array(len);
            for (let c = 0; c < ch; c++) {
                const data = audioBuffer.getChannelData(c);
                for (let i = 0; i < len; i++) out[i] += data[i];
            }
            for (let i = 0; i < len; i++) out[i] /= ch;
            return out;
        }

        // Downsample arbitrary input rate to 16k and quantize to Int16
        function downsampleTo16kInt16(float32Mono, inputSampleRate) {
            const targetRate = 16000;
            const ratio = inputSampleRate / targetRate;
            const newLength = Math.floor(float32Mono.length / ratio);
            const out = new Int16Array(newLength);
            let iOut = 0;
            for (let i = 0; i < newLength; i++) {
                const start = Math.floor(i * ratio);
                const end = Math.floor((i + 1) * ratio);
                let sum = 0;
                let count = 0;
                for (let j = start; j < end && j < float32Mono.length; j++) {
                    sum += float32Mono[j];
                    count++;
                }
                const sample = count ? (sum / count) : float32Mono[Math.min(start, float32Mono.length - 1)];
                const s = Math.max(-1, Math.min(1, sample));
                out[iOut++] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return out;
        }

        function encodeWav16kMono(int16Samples) {
            const numSamples = int16Samples.length;
            const headerSize = 44;
            const dataSize = numSamples * 2;
            const buffer = new ArrayBuffer(headerSize + dataSize);
            const view = new DataView(buffer);
            const writeString = (offset, str) => { for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i)); };
            writeString(0, 'RIFF');
            view.setUint32(4, 36 + dataSize, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true); // PCM chunk size
            view.setUint16(20, 1, true);  // PCM format
            view.setUint16(22, 1, true);  // channels
            view.setUint32(24, 16000, true); // sample rate
            view.setUint32(28, 16000 * 2, true); // byte rate
            view.setUint16(32, 2, true); // block align
            view.setUint16(34, 16, true); // bits per sample
            writeString(36, 'data');
            view.setUint32(40, dataSize, true);
            // PCM data
            let off = 44;
            for (let i = 0; i < numSamples; i++, off += 2) view.setInt16(off, int16Samples[i], true);
            return new Uint8Array(buffer);
        }

        function emitGroqSegmentIfReady(boundaryIndex) {
            const samplesSinceBoundary = boundaryIndex - groqLastBoundary;
            const msSinceBoundary = (samplesSinceBoundary / 16000) * 1000;
            if (!groqHadSpeech || msSinceBoundary < GROQ_MIN_SEGMENT_MS) return;
            const seg = groqSamples16k.slice(groqLastBoundary, boundaryIndex);
            const wavBytes = encodeWav16kMono(Int16Array.from(seg));
            console.log('[Segmenter] Emit segment', { samples: seg.length, ms: Math.round(msSinceBoundary), bytes: wavBytes.length });
            window.electronAPI.saveAudioSegment(wavBytes);
            groqLastBoundary = boundaryIndex;
            groqHadSpeech = false;
            groqInSilenceMs = 0;
            // Drop older samples to keep memory bounded
            if (groqLastBoundary > 0) {
                groqSamples16k = groqSamples16k.slice(groqLastBoundary);
                groqLastBoundary = 0;
            }
        }

        let beepSound;
        let clackSound;

        async function loadAudio() {
          try {
            // Try base64 first for packaged reliability
            const beepBase64 = await window.electronAPI.getAssetBase64('assets/audio/beep.mp3');
            if (beepBase64) {
              beepSound = new Audio(`data:audio/mpeg;base64,${beepBase64}`);
            } else {
              const beepUrl = await window.electronAPI.getAssetFileUrl('assets/audio/beep.mp3');
              console.warn('Beep base64 missing, using file URL', beepUrl);
              beepSound = new Audio(beepUrl);
            }

            const clackBase64 = await window.electronAPI.getAssetBase64('assets/audio/clack.mp3');
            if (clackBase64) {
              clackSound = new Audio(`data:audio/mpeg;base64,${clackBase64}`);
            } else {
              const clackUrl = await window.electronAPI.getAssetFileUrl('assets/audio/clack.mp3');
              console.warn('Clack base64 missing, using file URL', clackUrl);
              clackSound = new Audio(clackUrl);
            }

            if (beepSound) {
              beepSound.addEventListener('error', (e) => console.error('beep load/play error', e));
              try { beepSound.load(); } catch (_) {}
            }
            if (clackSound) {
              clackSound.addEventListener('error', (e) => console.error('clack load/play error', e));
              try { clackSound.load(); } catch (_) {}
            }
          } catch (error) {
            console.error('Error loading audio assets:', error);
            // Final fallback to file URLs
            try {
              const beepUrl = await window.electronAPI.getAssetFileUrl('assets/audio/beep.mp3');
              beepSound = new Audio(beepUrl);
              const clackUrl = await window.electronAPI.getAssetFileUrl('assets/audio/clack.mp3');
              clackSound = new Audio(clackUrl);
            } catch (e) {
              console.error('Unable to load audio cues via file URLs', e);
            }
          }
        }

        closeBtn.addEventListener('click', () => {
          window.electronAPI.closeWindow();
        });

        settingsIcon.addEventListener('click', () => {
          window.electronAPI.openSettingsWindow();
        });

        if (helpButton) {
            helpButton.addEventListener('click', () => {
                window.electronAPI.openExternalLink('https://github.com/3choff/dictate');
            });
        }
        if (sparkleButton) {
            sparkleButton.addEventListener('click', () => {
                if (!window.electronAPI) return;
                if (sparkleButton.classList.contains('loading')) {
                  // Abort in-flight request and stop pulsing immediately
                  if (typeof window.electronAPI.abortSparkle === 'function') {
                    window.electronAPI.abortSparkle();
                  }
                  sparkleButton.classList.remove('loading');
                  return;
                }
                // Start new request
                sparkleButton.classList.add('loading');
                if (typeof window.electronAPI.correctSelectionWithGemini === 'function') {
                  window.electronAPI.correctSelectionWithGemini();
                }
            });
        }

        if (window.electronAPI && typeof window.electronAPI.onSparkleDone === 'function') {
          window.electronAPI.onSparkleDone(() => {
            if (sparkleButton) sparkleButton.classList.remove('loading');
          });
        }

        // Global shortcut (Ctrl+Shift+G) handler from main process
        if (window.electronAPI && typeof window.electronAPI.onSparkleTrigger === 'function') {
          window.electronAPI.onSparkleTrigger(() => {
            if (sparkleButton) {
              sparkleButton.click();
            }
          });
        }

        window.electronAPI.onToggleMic(() => {
          micButton.click();
        });

        micButton.addEventListener('click', async () => {
          const isRecording = micButton.classList.contains('recording');

          if (isRecording) {
            if (clackSound) clackSound.play().catch((e) => console.error('clack play failed', e));
          } else {
            if (beepSound) beepSound.play().catch((e) => console.error('beep play failed', e));
          }

          micButton.classList.toggle('recording');

          if ((mediaRecorder && mediaRecorder.state === 'recording') || dgActive || groqActive) {
            try { mediaRecorder.stop(); } catch (e) { /* ignore */ }
            if (dgActive && dgSocket) {
              try { dgSocket.send(JSON.stringify({ type: 'CloseStream' })); } catch (_) {}
              try { dgSocket.close(); } catch (_) {}
              dgSocket = null;
              dgActive = false;
            }
            if (groqActive) {
              try {
                const currentIndex = groqSamples16k.length;
                const msSinceBoundary = ((currentIndex - groqLastBoundary) / 16000) * 1000;
                if (groqHadSpeech && msSinceBoundary >= GROQ_MIN_SEGMENT_MS) {
                  emitGroqSegmentIfReady(currentIndex);
                }
              } catch (_) {}
              try { if (groqProcessor) groqProcessor.disconnect(); } catch (_) {}
              try { if (groqSource) groqSource.disconnect(); } catch (_) {}
              try { if (groqAudioCtx) groqAudioCtx.close(); } catch (_) {}
              groqProcessor = null; groqSource = null; groqAudioCtx = null;
              groqActive = false;
              groqSamples16k = []; groqLastBoundary = 0; groqInSilenceMs = 0; groqHadSpeech = false;
            }
          } else {
            try {
              const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
              const settings = await window.electronAPI.getSettings();
              const dgKey = ((settings && (settings.deepgramApiKey || settings.apiKey)) || '').trim();
              const preserveFormatting = settings && settings.preserveFormatting !== false; // default true
              const useDeepgram = settings.apiService === 'deepgram' && dgKey.length > 0;
              const transcriptionLanguage = (settings && settings.transcriptionLanguage) || 'multilingual';
              const deepgramLanguage = (window.LanguageMap && window.LanguageMap.getDeepgramLanguage)
                ? window.LanguageMap.getDeepgramLanguage(transcriptionLanguage)
                : 'multi';

              audioChunks = [];

              if (useDeepgram) {
                console.log('[Deepgram] Streaming mode selected');
                dgActive = true;
                const pref = (window.Deepgram && Deepgram.pickPreferredMime) ? Deepgram.pickPreferredMime() : { mimeType: undefined, encoding: undefined };
                mediaRecorder = new MediaRecorder(stream, pref.mimeType ? { mimeType: pref.mimeType } : undefined);

                const wsUrl = (window.Deepgram && Deepgram.buildUrl)
                  ? Deepgram.buildUrl({ encoding: pref.encoding, smart_format: preserveFormatting, language: deepgramLanguage })
                  : 'wss://api.deepgram.com/v1/listen';
                try {
                  dgSocket = (window.Deepgram && Deepgram.createSocket) ? Deepgram.createSocket(dgKey, wsUrl) : new WebSocket(wsUrl, ['token', dgKey]);
                } catch (e) {
                  console.error('Deepgram WS open failed', e);
                  dgActive = false;
                }

                if (dgSocket) {
                  dgSocket.onopen = () => {
                    mediaRecorder.ondataavailable = (event) => {
                      if (event.data && event.data.size > 0 && dgSocket && dgSocket.readyState === WebSocket.OPEN) {
                        dgSocket.send(event.data);
                      }
                    };
                    try { mediaRecorder.start(250); } catch (e) { mediaRecorder.start(); }
                  };
                  dgSocket.onmessage = (event) => {
                    try {
                      const msg = JSON.parse(event.data);
                      if (msg.type === 'Results' && msg.channel && msg.channel.alternatives && msg.channel.alternatives[0]) {
                        const alt = msg.channel.alternatives[0];
                        if (alt.transcript && (msg.is_final || msg.speech_final)) {
                          const transcript = alt.transcript.trim();
                          const finalTranscript = transcript + ' ';
                          window.electronAPI.insertText(finalTranscript);
                        }
                      }
                    } catch (e) { /* ignore */ }
                  };
                  dgSocket.onerror = (e) => console.error('Deepgram WS error', e);
                  dgSocket.onclose = () => {
                    if (mediaRecorder && mediaRecorder.state === 'recording') {
                      try { mediaRecorder.stop(); } catch (_) {}
                    }
                    dgActive = false;
                    dgSocket = null;
                  };
                } else {
                  mediaRecorder = new MediaRecorder(stream);
                  mediaRecorder.ondataavailable = (event) => { audioChunks.push(event.data); };
                  mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    const audioBuffer = await audioBlob.arrayBuffer();
                    const audioData = new Uint8Array(audioBuffer);
                    window.electronAPI.saveAudio(audioData);
                  };
                  mediaRecorder.start();
                }
              } else {
                console.log('[Segmenter] Silence chunker mode selected');
                groqActive = true;
                groqAudioCtx = new (window.AudioContext || window.webkitAudioContext)();
                try { await groqAudioCtx.resume(); } catch (_) {}
                groqSource = groqAudioCtx.createMediaStreamSource(stream);
                const bufferSize = 4096; // ~85ms at 48k
                groqProcessor = groqAudioCtx.createScriptProcessor(bufferSize, 1, 1);
                let lastDebugLogTs = 0;
                groqProcessor.onaudioprocess = (ev) => {
                  const inBuf = ev.inputBuffer;
                  const mono = mixToMonoFloat32(inBuf);
                  let sumSq = 0;
                  for (let i = 0; i < mono.length; i++) { const s = mono[i]; sumSq += s * s; }
                  const rms = Math.sqrt(sumSq / Math.max(1, mono.length));
                  const db = dbfsFromRms(rms);
                  const frameMs = (mono.length / inBuf.sampleRate) * 1000;
                  if (db < GROQ_SILENCE_DB) {
                    groqInSilenceMs += frameMs;
                  } else {
                    groqInSilenceMs = 0;
                    groqHadSpeech = true;
                  }
                  const now = performance.now();
                  if (now - lastDebugLogTs > 1000) {
                    console.debug('[Segmenter] dB:', Math.round(db), 'silenceMs:', Math.round(groqInSilenceMs), 'hadSpeech:', groqHadSpeech);
                    lastDebugLogTs = now;
                  }
                  const int16 = downsampleTo16kInt16(mono, inBuf.sampleRate);
                  for (let i = 0; i < int16.length; i++) groqSamples16k.push(int16[i]);
                  const currentIndex = groqSamples16k.length;
                  const currentMsSinceBoundary = ((currentIndex - groqLastBoundary) / 16000) * 1000;
                  if (groqInSilenceMs >= GROQ_SILENCE_MS && groqHadSpeech) {
                    emitGroqSegmentIfReady(currentIndex);
                  } else if (groqHadSpeech && currentMsSinceBoundary >= GROQ_MAX_SEGMENT_MS) {
                    emitGroqSegmentIfReady(currentIndex);
                  }
                };
                // Important: connect nodes so onaudioprocess fires
                try { groqSource.connect(groqProcessor); } catch (_) {}
                try { groqProcessor.connect(groqAudioCtx.destination); } catch (_) {}
              }
            } catch (err) {
              console.error('Error accessing microphone:', err);
              micButton.classList.remove('recording');
            }
          }
        });

        loadAudio();
      });
    </script>
</body>
</html>
